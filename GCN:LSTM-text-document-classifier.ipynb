{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The implementation of Text GCN in the paper:\n## Liang Yao, Chengsheng Mao, Yuan Luo. \"Graph Convolutional Networks for Text Classification.\" In 33rd AAAI Conference on Artificial Intelligence (AAAI-19), 7370-7377","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re, os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-21T21:52:09.978129Z","iopub.execute_input":"2021-08-21T21:52:09.978998Z","iopub.status.idle":"2021-08-21T21:52:11.102599Z","shell.execute_reply.started":"2021-08-21T21:52:09.978858Z","shell.execute_reply":"2021-08-21T21:52:11.101721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"PATH= '../input/bbc-full-text-document-classification/bbc-fulltext (document classification)/bbc/' \n\ndef readFile(full_file_name):\n    with open(full_file_name, 'r', encoding=\"utf-8\", errors='ignore') as f:\n        return '\\n'.join(f.readlines())\n#retreives data from the PATH and put it list 'data' in the format of 'category' and 'text'\ndef getData():\n\n    data = []\n    \n    for root, dirs, files in os.walk(PATH):\n        for f in files:\n            if os.path.splitext(f)[1] == '.txt':\n                full_file_name = os.path.join(root, f)\n                category = os.path.basename(root)\n                data.append({'category':category,'text':readFile(full_file_name)})\n                    \n    return data\n\ndata_frame = pd.DataFrame(getData())\nprint(data_frame.head(-5))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:18:46.147279Z","iopub.execute_input":"2021-08-21T22:18:46.147847Z","iopub.status.idle":"2021-08-21T22:18:47.048563Z","shell.execute_reply.started":"2021-08-21T22:18:46.147797Z","shell.execute_reply":"2021-08-21T22:18:47.047546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove and clean Docs/Words","metadata":{}},{"cell_type":"code","source":"# cleans 'text' of any unnessary content\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:18:47.115686Z","iopub.execute_input":"2021-08-21T22:18:47.116055Z","iopub.status.idle":"2021-08-21T22:18:47.12468Z","shell.execute_reply.started":"2021-08-21T22:18:47.116026Z","shell.execute_reply":"2021-08-21T22:18:47.123898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\nfrom nltk.wsd import lesk\nfrom nltk.corpus import wordnet as wn\n\nstoplist = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:18:48.002468Z","iopub.execute_input":"2021-08-21T22:18:48.002991Z","iopub.status.idle":"2021-08-21T22:18:48.66181Z","shell.execute_reply.started":"2021-08-21T22:18:48.002958Z","shell.execute_reply":"2021-08-21T22:18:48.660519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Words","metadata":{}},{"cell_type":"code","source":"#take all the words in the documents and place it in this list below. \ndoc_content_list = []#should have line by line of all the documents in a list\n\nfor root, dirs, files in os.walk(PATH):\n\n    for f in files:\n        if os.path.splitext(f)[1] == '.txt':\n            with open(root+'/'+f, 'r', errors='replace')as f:\n                lines = f.readlines()\n                for line in lines:\n                    doc_content_list.append(line.strip())\n    \n'''\nfor texts in data_frame[[\"text\"]]:\n    text = data_frame[texts]\n    for doc in text.values: \n        lines = doc.split('\\n')\n        for line in lines:\n            doc_content_list.append(line)'''\n        \n    \n#print(len(doc_content_list))\nprint(doc_content_list[2])\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:18:49.744575Z","iopub.execute_input":"2021-08-21T22:18:49.744935Z","iopub.status.idle":"2021-08-21T22:18:50.612187Z","shell.execute_reply.started":"2021-08-21T22:18:49.744903Z","shell.execute_reply":"2021-08-21T22:18:50.611218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_freq = {}  # to remove rare words\n\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n            \nprint(len(word_freq))\nimport itertools \nout = dict(itertools.islice(word_freq.items(), 5)) \nprint(str(out))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:18:51.457712Z","iopub.execute_input":"2021-08-21T22:18:51.458068Z","iopub.status.idle":"2021-08-21T22:18:53.159546Z","shell.execute_reply.started":"2021-08-21T22:18:51.458039Z","shell.execute_reply":"2021-08-21T22:18:53.153797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BBC clean text","metadata":{}},{"cell_type":"code","source":"#here we use stop_words to get rid of unuseful words\nclean_docs = []\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    doc_words = []\n    for word in words:\n        # word not in stop_words and word_freq[word] >= 5\n        if word not in stoplist and word_freq[word] >= 5:\n            doc_words.append(word)\n\n    doc_str = ' '.join(doc_words).strip()\n    if doc_str != '':\n        clean_docs.append(doc_str)\nprint(clean_docs[328])\nclean_corpus_str = '\\n'.join(clean_docs)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:14.636925Z","iopub.execute_input":"2021-08-21T22:19:14.637287Z","iopub.status.idle":"2021-08-21T22:19:16.360083Z","shell.execute_reply.started":"2021-08-21T22:19:14.637258Z","shell.execute_reply":"2021-08-21T22:19:16.35922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data into Train and Test","metadata":{}},{"cell_type":"code","source":"train_x, train_y, test_x, test_y = model_selection.train_test_split(data_frame.text, data_frame.category, test_size=0.20)\n\ndata_frame.category[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:18.052717Z","iopub.execute_input":"2021-08-21T22:19:18.053078Z","iopub.status.idle":"2021-08-21T22:19:18.064656Z","shell.execute_reply.started":"2021-08-21T22:19:18.053049Z","shell.execute_reply":"2021-08-21T22:19:18.063646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a list of lists with data details of all documents. Each list have the \"document number\", train or test, and category","metadata":{}},{"cell_type":"code","source":"trainx= pd.DataFrame(train_x)\ntesty= pd.DataFrame(test_y)\n\n\ndoc_name_list=[]\ndoc_train_list = []\ndoc_test_list = []\ntrain_ids = []\ntest_ids = []\n\nfor doc, text in trainx.iterrows():\n    category = data_frame.iloc[doc][0]\n    lists=[]\n    lists.extend([doc, 'train', category])\n    doc_name_list.append(lists)\n    doc_train_list.append(lists)\n    train_ids.append(doc)\n    \nfor doc, text in testy.iterrows():\n    category = data_frame.iloc[doc][0]\n    lists=[]\n    lists.extend([doc, 'test', category])\n    doc_name_list.append(lists)\n    doc_test_list.append(lists)\n    test_ids.append(doc)\n\n\n#print(len(doc_name_list))\n#print(len(doc_train_list))\n#print(len(doc_test_list))\n        \n    \n\n#shuffle\nimport random\nrandom.shuffle(doc_train_list)\nrandom.shuffle(doc_test_list)\nrandom.shuffle(train_ids)\nrandom.shuffle(test_ids)\n    \ntest_ids_str = '\\n'.join(str(index) for index in test_ids)\ntrain_ids_str = '\\n'.join(str(index) for index in train_ids)\nids = train_ids + test_ids\n\n\nshuffle_doc_name_list = []\nshuffle_doc_words_list = []\nfor id in ids:\n    shuffle_doc_name_list.append(doc_name_list[int(id)])\n    shuffle_doc_words_list.append(clean_docs[int(id)])\n\n\nList=[]\nfor x,y,z in shuffle_doc_name_list:\n    L = f\"{x}\\t{y}\\t{z}\"\n    List.append(L)\nshuffle_doc_name_str = '\\n'.join(List)\nshuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:19.760851Z","iopub.execute_input":"2021-08-21T22:19:19.761263Z","iopub.status.idle":"2021-08-21T22:19:20.155251Z","shell.execute_reply.started":"2021-08-21T22:19:19.761232Z","shell.execute_reply":"2021-08-21T22:19:20.154182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Vocab","metadata":{}},{"cell_type":"code","source":"word_freq = {}\nword_set = set()\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    for word in words:\n        word_set.add(word)\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n              \nvocab = list(word_set)\nvocab_size = len(vocab)\nprint(vocab_size)\n\nx = [word for word in word_set]\n\nl=[x for x in word_freq.items()]\nprint(l[0:10])\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:21.067895Z","iopub.execute_input":"2021-08-21T22:19:21.068306Z","iopub.status.idle":"2021-08-21T22:19:21.132674Z","shell.execute_reply.started":"2021-08-21T22:19:21.068272Z","shell.execute_reply":"2021-08-21T22:19:21.131554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Word-Document list","metadata":{}},{"cell_type":"code","source":"word_doc_list = {}\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    appeared = set()\n    for word in words:\n        if word in appeared:\n            continue\n        if word in word_doc_list:\n            doc_list = word_doc_list[word]\n            doc_list.append(i)\n            word_doc_list[word] = doc_list\n        else:\n            word_doc_list[word] = [i]\n        appeared.add(word)\n\n#print(len(word_doc_list))\n#print(len(appeared))\n\n\n\nl=[x for x in word_doc_list.items()]\nprint(l[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:22.229348Z","iopub.execute_input":"2021-08-21T22:19:22.229712Z","iopub.status.idle":"2021-08-21T22:19:22.438691Z","shell.execute_reply.started":"2021-08-21T22:19:22.229682Z","shell.execute_reply":"2021-08-21T22:19:22.437637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Word-Document frequency dictionary","metadata":{}},{"cell_type":"code","source":"word_doc_freq = {}\nfor word, doc_list in word_doc_list.items():\n    word_doc_freq[word] = len(doc_list)\n    \nl=[x for x in word_doc_freq.items()]\nprint(l[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:23.861753Z","iopub.execute_input":"2021-08-21T22:19:23.862126Z","iopub.status.idle":"2021-08-21T22:19:23.873449Z","shell.execute_reply.started":"2021-08-21T22:19:23.862095Z","shell.execute_reply":"2021-08-21T22:19:23.872304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Word ID Map","metadata":{}},{"cell_type":"code","source":"word_id_map = {}\nfor i in range(vocab_size):\n    word_id_map[vocab[i]] = i\n\nvocab_str = '\\n'.join(vocab)\n\nl=[x for x in word_id_map.items()]\nprint(l[0:10])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:25.090951Z","iopub.execute_input":"2021-08-21T22:19:25.091302Z","iopub.status.idle":"2021-08-21T22:19:25.102887Z","shell.execute_reply.started":"2021-08-21T22:19:25.091271Z","shell.execute_reply":"2021-08-21T22:19:25.101674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Document-Word edges","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndefinitions = []\nfor word in vocab:\n    word = word.strip()\n    synsets = wn.synsets(clean_str(word))\n    word_defs = []\n    for synset in synsets:\n        syn_def = synset.definition()\n        word_defs.append(syn_def)\n    word_des = ' '.join(word_defs)\n    if word_des == '':\n        word_des = '<PAD>'\n    definitions.append(word_des)\n\ntfidf_vec = TfidfVectorizer(max_features=1000)\ntfidf_matrix = tfidf_vec.fit_transform(definitions)\ntfidf_matrix_array = tfidf_matrix.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:28.171578Z","iopub.execute_input":"2021-08-21T22:19:28.171967Z","iopub.status.idle":"2021-08-21T22:19:34.83871Z","shell.execute_reply.started":"2021-08-21T22:19:28.171933Z","shell.execute_reply":"2021-08-21T22:19:34.837807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Vectors","metadata":{}},{"cell_type":"code","source":"\nword_vectors = []\nfor i in range(len(vocab)):\n    word = vocab[i]\n    vector = tfidf_matrix_array[i]\n    str_vector = []\n    for j in range(len(vector)):\n        str_vector.append(str(vector[j]))\n    temp = ' '.join(str_vector)\n    word_vector = word + ' ' + temp\n    word_vectors.append(word_vector)\nprint(len(word_vectors))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:19:40.7905Z","iopub.execute_input":"2021-08-21T22:19:40.791048Z","iopub.status.idle":"2021-08-21T22:19:45.982013Z","shell.execute_reply.started":"2021-08-21T22:19:40.791015Z","shell.execute_reply":"2021-08-21T22:19:45.980943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec","metadata":{}},{"cell_type":"code","source":"\nword_vector_map = {}\nvocab =[]\nembd=[]\n\nfor x in word_vectors:\n    line = x.strip().split(' ')\n    if (len(line) > 2):\n        vocab.append(line[0])\n        vector= line[1:]\n        length = len(vector)\n        for i in range(length):\n            vector[i] = float(vector[i])\n        embd.append(vector)\n        word_vector_map[line[0]] = vector\n\nprint(vocab[0], embd[0], len(word_vectors))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:02.538311Z","iopub.execute_input":"2021-08-21T22:20:02.5387Z","iopub.status.idle":"2021-08-21T22:20:05.780768Z","shell.execute_reply.started":"2021-08-21T22:20:02.538669Z","shell.execute_reply":"2021-08-21T22:20:05.77987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Category Label list","metadata":{}},{"cell_type":"code","source":"label_set = set()\nfor doc_meta in shuffle_doc_name_list:\n    label_set.add(doc_meta[2])\nlabel_list = list(label_set)\n\nlabel_list_str = '\\n'.join(label_list)\nprint(label_list_str)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:07.594112Z","iopub.execute_input":"2021-08-21T22:20:07.594497Z","iopub.status.idle":"2021-08-21T22:20:07.60271Z","shell.execute_reply.started":"2021-08-21T22:20:07.594465Z","shell.execute_reply":"2021-08-21T22:20:07.601302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build training variables, we select only 90% of the training set","metadata":{}},{"cell_type":"code","source":"train_size = len(train_ids)\nval_size = int(0.1 * train_size)\nreal_train_size = train_size - val_size  # - int(0.5 * train_size)\n# different training rates\n\nreal_train_doc_names = shuffle_doc_name_list[:real_train_size]\nList2=[]\nfor x,y,z in real_train_doc_names:\n    L = f\"{x}\\t{y}\\t{z}\"\n    List2.append(L)\nreal_train_doc_names_str = '\\n'.join(List2)\n\nprint(shuffle_doc_words_list[2])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:11.02311Z","iopub.execute_input":"2021-08-21T22:20:11.023484Z","iopub.status.idle":"2021-08-21T22:20:11.033227Z","shell.execute_reply.started":"2021-08-21T22:20:11.023453Z","shell.execute_reply":"2021-08-21T22:20:11.032045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Graph\n#### (This part will take a long time)","metadata":{}},{"cell_type":"code","source":"import time\nstart = time.process_time()\nword_embeddings_dim = len(embd[0])\n\n\nrow_x = []\ncol_x = []\ndata_x = []\nfor i in range(real_train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            #print(word_vector)\n            #print(np.array(word_vector).shape, doc_vec.shape)\n            doc_vec = doc_vec = np.array(word_vector)\n    for j in range(word_embeddings_dim):\n        row_x.append(i)\n        col_x.append(j)\n        data_x.append(doc_vec[j]/doc_len)\n\n\nprint(\"--- %s seconds ---\" % (time.process_time() - start))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:13.470374Z","iopub.execute_input":"2021-08-21T22:20:13.471014Z","iopub.status.idle":"2021-08-21T22:20:24.068837Z","shell.execute_reply.started":"2021-08-21T22:20:13.470981Z","shell.execute_reply":"2021-08-21T22:20:24.067827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create _ matrix*","metadata":{}},{"cell_type":"code","source":"import scipy.sparse as sp\nx = sp.csr_matrix((data_x, (row_x, col_x)), shape=(real_train_size, word_embeddings_dim))\ny=[]#matrix of one hot vectors\n\nfor i in range(real_train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    label = doc_meta[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    y.append(one_hot)\ny = np.array(y)\nprint(y[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:26.556823Z","iopub.execute_input":"2021-08-21T22:20:26.557213Z","iopub.status.idle":"2021-08-21T22:20:28.222854Z","shell.execute_reply.started":"2021-08-21T22:20:26.557181Z","shell.execute_reply":"2021-08-21T22:20:28.222053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Vectors of test docs, no initial features","metadata":{}},{"cell_type":"code","source":"test_size = len(test_ids)\n\nrow_test_x = []\ncol_test_x = []\ndata_test_x = []\n\nfor i in range(test_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i + train_size]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_test_x.append(i)\n        col_test_x.append(j)\n        data_test_x.append(doc_vec[j] / doc_len)\nprint(len(data_test_x))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:30.909235Z","iopub.execute_input":"2021-08-21T22:20:30.909804Z","iopub.status.idle":"2021-08-21T22:20:34.073417Z","shell.execute_reply.started":"2021-08-21T22:20:30.909764Z","shell.execute_reply":"2021-08-21T22:20:34.072309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = sp.csr_matrix((data_test_x, (row_test_x, col_test_x)),\n                   shape=(test_size, word_embeddings_dim))\ntest_y =[]\nfor i in range(test_size):\n    doc_meta = shuffle_doc_name_list[i + train_size]\n    label = doc_meta[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    test_y.append(one_hot)\ntest_y = np.array(test_y)\nprint(test_y[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:35.344346Z","iopub.execute_input":"2021-08-21T22:20:35.344741Z","iopub.status.idle":"2021-08-21T22:20:35.775477Z","shell.execute_reply.started":"2021-08-21T22:20:35.344699Z","shell.execute_reply":"2021-08-21T22:20:35.774435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create feature vectors of both labeled and unlabeled training instances,(a superset of x)\n### This part will take longer","metadata":{}},{"cell_type":"code","source":"start2 = time.process_time()\n\nword_vectors = np.random.uniform(-0.01, 0.01,\n                                 (vocab_size, word_embeddings_dim))\nfor i in range(len(vocab)):\n    word = vocab[i]\n    if word in word_vector_map:\n        vector = word_vector_map[word]\n        word_vectors[i] = vector\n\nrow_allx = []\ncol_allx = []\ndata_allx = []\n\nfor i in range(train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i))\n        col_allx.append(j)\n        data_allx.append(doc_vec[j]/doc_len)\nfor i in range(vocab_size):\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i+train_size))\n        col_allx.append(j)\n        data_allx.append(word_vectors.item((i, j)))\n        \n\n        \nallx = sp.csr_matrix(\n    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n\nprint(\"--- %s seconds ---\" % (time.process_time() - start2))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:20:37.391093Z","iopub.execute_input":"2021-08-21T22:20:37.391456Z","iopub.status.idle":"2021-08-21T22:21:04.327733Z","shell.execute_reply.started":"2021-08-21T22:20:37.391426Z","shell.execute_reply":"2021-08-21T22:21:04.326388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ally=[]\nfor i in range(train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    label = doc_meta[2]\n    one_hot = [0 for l in range(len(label_list))] \n    label_index = label_list.index(label)\n    one_hot[label_index]=1\n    ally.append(one_hot)\n    \nfor i in range(vocab_size):\n    one_hot = [0 for l in range(len(label_list))]\n    ally.append(one_hot)\n    \nally = np.array(ally)\nprint(x.shape, y.shape, test_x.shape, test_y.shape, allx.shape, ally.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:07.841766Z","iopub.execute_input":"2021-08-21T22:21:07.842153Z","iopub.status.idle":"2021-08-21T22:21:08.7402Z","shell.execute_reply.started":"2021-08-21T22:21:07.84212Z","shell.execute_reply":"2021-08-21T22:21:08.739074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word co-occurence with context windows\n### This will take some time","metadata":{}},{"cell_type":"code","source":"start3 = time.process_time()\nwindow_size = 20\nwindows = []\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    length = len(words)\n    if length <= window_size:\n        windows.append(words)\n    else:\n        # print(length, length - window_size + 1)\n        for j in range(length - window_size + 1):\n            window = words[j: j + window_size]\n            windows.append(window)\n            # print(window)\nword_window_freq = {}\nfor window in windows:\n    appeared = set()\n    for i in range(len(window)):\n        if window[i] in appeared:\n            continue\n        if window[i] in word_window_freq:\n            word_window_freq[window[i]] += 1\n        else:\n            word_window_freq[window[i]] = 1\n        appeared.add(window[i])\n\nword_pair_count = {}\nfor window in windows:\n    for i in range(1, len(window)):\n        for j in range(0, i):\n            word_i = window[i]\n            word_i_id = word_id_map[word_i]\n            word_j = window[j]\n            word_j_id = word_id_map[word_j]\n            if word_i_id == word_j_id:\n                continue\n            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n            # two orders\n            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n                \nprint(len(word_pair_count))\nprint(\"--- %s seconds ---\" % (time.process_time() - start3))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:09.681671Z","iopub.execute_input":"2021-08-21T22:21:09.682078Z","iopub.status.idle":"2021-08-21T22:21:42.215441Z","shell.execute_reply.started":"2021-08-21T22:21:09.682046Z","shell.execute_reply":"2021-08-21T22:21:42.21423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pointwise mutual information (PMI) as weights","metadata":{}},{"cell_type":"code","source":"from math import log\n\nrow = []\ncol = []\nweight = []\n\nnum_window = len(windows)\n\nfor key in word_pair_count:\n    temp = key.split(',')\n    i = int(temp[0])\n    j = int(temp[1])\n    count = word_pair_count[key]\n    word_freq_i = word_window_freq[vocab[i]]\n    word_freq_j = word_window_freq[vocab[j]]\n    pmi = log((1.0 * count / num_window) /\n              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n    if pmi <= 0:\n        continue\n    row.append(train_size + i)\n    col.append(train_size + j)\n    weight.append(pmi)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:48.039607Z","iopub.execute_input":"2021-08-21T22:21:48.040166Z","iopub.status.idle":"2021-08-21T22:21:52.043975Z","shell.execute_reply.started":"2021-08-21T22:21:48.04012Z","shell.execute_reply":"2021-08-21T22:21:52.042935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## word vector cosine similarity as weights\n### This may take up to 91.9 minutes!","metadata":{}},{"cell_type":"code","source":"'''from scipy.spatial.distance import cosine\nstart4 = time.process_time()\n\n\nfor i in range(vocab_size):\n    for j in range(vocab_size):\n        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n            vector_i = np.array(word_vector_map[vocab[i]])\n            vector_j = np.array(word_vector_map[vocab[j]])\n            similarity = 1.0 - cosine(vector_i, vector_j)\n            if similarity > 0.9:\n                #print(vocab[i], vocab[j], similarity)\n                row.append(train_size + i)\n                col.append(train_size + j)\n                weight.append(similarity)\nprint(\"--- %s seconds ---\" % (time.process_time() - start4))''' ","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:54.027556Z","iopub.execute_input":"2021-08-21T22:21:54.027947Z","iopub.status.idle":"2021-08-21T22:21:54.034699Z","shell.execute_reply.started":"2021-08-21T22:21:54.027915Z","shell.execute_reply":"2021-08-21T22:21:54.033636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doc Word Frequency","metadata":{}},{"cell_type":"code","source":"doc_word_freq = {}\n\nfor doc_id in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[doc_id]\n    words = doc_words.split()\n    for word in words:\n        word_id = word_id_map[word]\n        doc_word_str = str(doc_id) + ',' + str(word_id)\n        if doc_word_str in doc_word_freq:\n            doc_word_freq[doc_word_str] += 1\n        else:\n            doc_word_freq[doc_word_str] = 1\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_word_set = set()\n    for word in words:\n        if word in doc_word_set:\n            continue\n        j = word_id_map[word]\n        key = str(i) + ',' + str(j)\n        freq = doc_word_freq[key]\n        if i < train_size:\n            row.append(i)\n        else:\n            row.append(i + vocab_size)\n        col.append(train_size + j)\n        idf = log(1.0 * len(shuffle_doc_words_list) /\n                  word_doc_freq[vocab[j]])\n        weight.append(freq * idf)\n        doc_word_set.add(word)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:55.445108Z","iopub.execute_input":"2021-08-21T22:21:55.445496Z","iopub.status.idle":"2021-08-21T22:21:55.867833Z","shell.execute_reply.started":"2021-08-21T22:21:55.445463Z","shell.execute_reply":"2021-08-21T22:21:55.866767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Nodes and Adjency Matrix with the proper weights","metadata":{}},{"cell_type":"code","source":"node_size = train_size + vocab_size + test_size\nadj = sp.csr_matrix(\n    (weight, (row, col)), shape=(node_size, node_size))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:57.385838Z","iopub.execute_input":"2021-08-21T22:21:57.386188Z","iopub.status.idle":"2021-08-21T22:21:58.390967Z","shell.execute_reply.started":"2021-08-21T22:21:57.386159Z","shell.execute_reply":"2021-08-21T22:21:58.39Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nseed = random.randint(1, 200)\nseed = 2019\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:21:59.45017Z","iopub.execute_input":"2021-08-21T22:21:59.450542Z","iopub.status.idle":"2021-08-21T22:22:00.750935Z","shell.execute_reply.started":"2021-08-21T22:21:59.450495Z","shell.execute_reply":"2021-08-21T22:22:00.749817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Docstring for CONFIG","metadata":{}},{"cell_type":"code","source":"class CONFIG(object):\n    \"\"\"docstring for CONFIG\"\"\"\n    def __init__(self):\n        super(CONFIG, self).__init__()\n        \n        self.dataset = 'bbc'\n        self.model = 'gcn'  # 'gcn', 'gcn_cheby', 'dense'\n        self.learning_rate = 0.02   # Initial learning rate.\n        self.epochs  = 200  # Number of epochs to train.\n        self.hidden1 = 200  # Number of units in hidden layer 1.\n        self.dropout = 0.5  # Dropout rate (1 - keep probability).\n        self.weight_decay = 0.   # Weight for L2 loss on embedding matrix.\n        self.early_stopping = 10 # Tolerance for early stopping (# of epochs).\n        self.max_degree = 3      # Maximum Chebyshev polynomial degree.","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:01.663133Z","iopub.execute_input":"2021-08-21T22:22:01.663495Z","iopub.status.idle":"2021-08-21T22:22:01.67027Z","shell.execute_reply.started":"2021-08-21T22:22:01.663464Z","shell.execute_reply":"2021-08-21T22:22:01.669133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multilayer Perceptron","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, input_dim, dropout_rate=0., num_classes=10):\n        super(MLP, self).__init__()\n\n        self.fc1 = nn.Linear(input_dim, 200)\n        self.fc2 = nn.Linear(200, num_classes)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(dropout_rate)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n\n        out = self.fc2(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:02.778174Z","iopub.execute_input":"2021-08-21T22:22:02.778519Z","iopub.status.idle":"2021-08-21T22:22:02.787829Z","shell.execute_reply.started":"2021-08-21T22:22:02.77849Z","shell.execute_reply":"2021-08-21T22:22:02.786668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graph Convolutional Network\\Convolution","metadata":{}},{"cell_type":"code","source":"class GraphConvolution(nn.Module):\n    def __init__( self, input_dim, \\\n                        output_dim, \\\n                        support, \\\n                        act_func = None, \\\n                        featureless = False, \\\n                        dropout_rate = 0., \\\n                        bias=False):\n        super(GraphConvolution, self).__init__()\n        self.support = support\n        self.featureless = featureless\n\n        for i in range(len(self.support)):\n            setattr(self, 'W{}'.format(i), nn.Parameter(torch.randn(input_dim, output_dim)))\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(1, output_dim))\n\n        self.act_func = act_func\n        self.dropout = nn.Dropout(dropout_rate)\n\n        \n    def forward(self, x):\n        x = self.dropout(x)\n\n        for i in range(len(self.support)):\n            if self.featureless:\n                pre_sup = getattr(self, 'W{}'.format(i))\n            else:\n                pre_sup = x.mm(getattr(self, 'W{}'.format(i)))\n            \n            if i == 0:\n                out = self.support[i].mm(pre_sup)\n            else:\n                out += self.support[i].mm(pre_sup)\n\n        if self.act_func is not None:\n            out = self.act_func(out)\n\n        self.embedding = out\n        return out\n\n\nclass GCN(nn.Module):\n    def __init__( self, input_dim, \\\n                        support,\\\n                        dropout_rate=0., \\\n                        num_classes=10):\n        super(GCN, self).__init__()\n        \n        # GraphConvolution\n        self.layer1 = GraphConvolution(input_dim, 200, support, act_func=nn.ReLU(), featureless=True, dropout_rate=dropout_rate)\n        self.layer2 = GraphConvolution(200, num_classes, support, dropout_rate=dropout_rate)\n        \n    \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:03.544401Z","iopub.execute_input":"2021-08-21T22:22:03.544791Z","iopub.status.idle":"2021-08-21T22:22:03.5592Z","shell.execute_reply.started":"2021-08-21T22:22:03.544759Z","shell.execute_reply":"2021-08-21T22:22:03.557945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data/Corpus","metadata":{}},{"cell_type":"code","source":"#def load_data(x, y, tx, ty, allx, ally):\nx, y, test_x, test_y, allx, ally, adj = tuple([x,y,test_x,test_y,allx,ally,adj])\n\nfeatures = sp.vstack((allx, test_x)).tolil()\nlabels = np.vstack((ally, test_y))\nval_size= train_size - x.shape[0]\ntest_size = test_x.shape[0]\nidx_train = range(len(y))\nidx_val = range(len(y), len(y) + val_size)\nidx_test = range(allx.shape[0], allx.shape[0] + test_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:04.257724Z","iopub.execute_input":"2021-08-21T22:22:04.258066Z","iopub.status.idle":"2021-08-21T22:22:05.206605Z","shell.execute_reply.started":"2021-08-21T22:22:04.258037Z","shell.execute_reply":"2021-08-21T22:22:05.205561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creat Sample Mask","metadata":{}},{"cell_type":"code","source":"def sample_mask(idx, l):\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:05.20801Z","iopub.execute_input":"2021-08-21T22:22:05.208311Z","iopub.status.idle":"2021-08-21T22:22:05.21355Z","shell.execute_reply.started":"2021-08-21T22:22:05.208283Z","shell.execute_reply":"2021-08-21T22:22:05.212511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mask = sample_mask(idx_train, labels.shape[0])\nval_mask = sample_mask(idx_val, labels.shape[0])\ntest_mask = sample_mask(idx_test, labels.shape[0])\n\ny_train = np.zeros(labels.shape)\ny_val = np.zeros(labels.shape)\ny_test = np.zeros(labels.shape)\ny_train[train_mask, :] = labels[train_mask, :]\ny_val[val_mask, :] = labels[val_mask, :]\ny_test[test_mask, :] = labels[test_mask, :]\n\nadj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:05.322282Z","iopub.execute_input":"2021-08-21T22:22:05.322656Z","iopub.status.idle":"2021-08-21T22:22:05.413107Z","shell.execute_reply.started":"2021-08-21T22:22:05.322625Z","shell.execute_reply":"2021-08-21T22:22:05.412211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing","metadata":{}},{"cell_type":"code","source":"def preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    # return sparse_to_tuple(features)\n    return features.A\n\ndef normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\ndef preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    # return sparse_to_tuple(adj_normalized)\n    return adj_normalized.A\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:06.934208Z","iopub.execute_input":"2021-08-21T22:22:06.934611Z","iopub.status.idle":"2021-08-21T22:22:06.943917Z","shell.execute_reply.started":"2021-08-21T22:22:06.934574Z","shell.execute_reply":"2021-08-21T22:22:06.94273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing","metadata":{}},{"cell_type":"code","source":"features = sp.identity(features.shape[0])\nfeatures = preprocess_features(features)\n\nsupport = [preprocess_adj(adj)]\nnum_supports = 1\nmodel_func = GCN","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:08.733611Z","iopub.execute_input":"2021-08-21T22:22:08.734013Z","iopub.status.idle":"2021-08-21T22:22:09.689979Z","shell.execute_reply.started":"2021-08-21T22:22:08.733978Z","shell.execute_reply":"2021-08-21T22:22:09.688961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Placeholders","metadata":{}},{"cell_type":"code","source":"t_features = torch.from_numpy(features)\nt_y_train = torch.from_numpy(y_train)\nt_y_val = torch.from_numpy(y_val)\nt_y_test = torch.from_numpy(y_test)\nt_train_mask = torch.from_numpy(train_mask.astype(np.float32))\ntm_train_mask = torch.transpose(torch.unsqueeze(t_train_mask, 0), 1, 0).repeat(1, y_train.shape[1])\n\nt_support = []\nfor i in range(len(support)):\n    t_support.append(torch.Tensor(support[i]))\n\nmodel = model_func(input_dim=features.shape[0], support=t_support, num_classes=y_train.shape[1])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:10.015631Z","iopub.execute_input":"2021-08-21T22:22:10.016019Z","iopub.status.idle":"2021-08-21T22:22:10.285157Z","shell.execute_reply.started":"2021-08-21T22:22:10.015985Z","shell.execute_reply":"2021-08-21T22:22:10.284262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"cfg=CONFIG()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:11.533885Z","iopub.execute_input":"2021-08-21T22:22:11.534466Z","iopub.status.idle":"2021-08-21T22:22:11.540463Z","shell.execute_reply.started":"2021-08-21T22:22:11.534414Z","shell.execute_reply":"2021-08-21T22:22:11.539759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model and Evalution Function","metadata":{}},{"cell_type":"code","source":"def evaluate(features, labels, mask):\n    t_test = time.time()\n    model.eval()\n    with torch.no_grad():\n        logits = model(features)\n        t_mask = torch.from_numpy(np.array(mask*1., dtype=np.float32))\n        tm_mask = torch.transpose(torch.unsqueeze(t_mask, 0), 1, 0).repeat(1, labels.shape[1])\n        loss = criterion(logits * tm_mask, torch.max(labels, 1)[1])\n        pred = torch.max(logits, 1)[1]\n        acc = ((pred == torch.max(labels, 1)[1]).float() * t_mask).sum().item() / t_mask.sum().item()\n        \n    return loss.numpy(), acc, pred.numpy(), labels.numpy(), (time.time() - t_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:12.546571Z","iopub.execute_input":"2021-08-21T22:22:12.547062Z","iopub.status.idle":"2021-08-21T22:22:12.554539Z","shell.execute_reply.started":"2021-08-21T22:22:12.547031Z","shell.execute_reply":"2021-08-21T22:22:12.553743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\ndef print_log(msg='', end='\\n'):\n    now = datetime.datetime.now()\n    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' ' \\\n      + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n\n    if isinstance(msg, str):\n        lines = msg.split('\\n')\n    else:\n        lines = [msg]\n        \n    for line in lines:\n        if line == lines[-1]:\n            print('[' + t + '] ' + str(line), end=end)\n        else: \n            print('[' + t + '] ' + str(line))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:13.337563Z","iopub.execute_input":"2021-08-21T22:22:13.338143Z","iopub.status.idle":"2021-08-21T22:22:13.345099Z","shell.execute_reply.started":"2021-08-21T22:22:13.338107Z","shell.execute_reply":"2021-08-21T22:22:13.344353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"code","source":"val_losses = []\nfor epoch in range(cfg.epochs):\n\n    t = time.time()\n    \n    # Forward pass\n    logits = model(t_features)\n    loss = criterion(logits * tm_train_mask, torch.max(t_y_train, 1)[1])    \n    acc = ((torch.max(logits, 1)[1] == torch.max(t_y_train, 1)[1]).float() * t_train_mask).sum().item() / t_train_mask.sum().item()\n        \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Validation\n    val_loss, val_acc, pred, labels, duration = evaluate(t_features, t_y_val, val_mask)\n    val_losses.append(val_loss)\n\n    print_log(\"Epoch: {:.0f}, train_loss= {:.5f}, train_acc= {:.5f}, val_loss= {:.5f}, val_acc= {:.5f}, time= {:.5f}\"\\\n                .format(epoch + 1, loss, acc, val_loss, val_acc, time.time() - t))\n\n    if epoch > cfg.early_stopping and val_losses[-1] > np.mean(val_losses[-(cfg.early_stopping+1):-1]):\n        print_log(\"Early stopping...\")\n        break\n\n\nprint_log(\"Optimization Finished!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:14.467247Z","iopub.execute_input":"2021-08-21T22:22:14.467628Z","iopub.status.idle":"2021-08-21T22:22:25.705862Z","shell.execute_reply.started":"2021-08-21T22:22:14.467592Z","shell.execute_reply":"2021-08-21T22:22:25.704812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\ntest_loss, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, test_mask)\nprint_log(\"Test set results: \\n\\t loss= {:.5f}, accuracy= {:.5f}, time= {:.5f}\".format(test_loss, test_acc, test_duration))\n\ntest_pred = []\ntest_labels = []\nfor i in range(len(test_mask)):\n    if test_mask[i]:\n        test_pred.append(pred[i])\n        test_labels.append(np.argmax(labels[i]))\n\n\nprint_log(\"Test Precision, Recall and F1-Score...\")\nprint_log(metrics.classification_report(test_labels, test_pred, digits=4))\nprint_log(\"Macro average Test Precision, Recall and F1-Score...\")\nprint_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\nprint_log(\"Micro average Test Precision, Recall and F1-Score...\")\nprint_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T22:22:28.381406Z","iopub.execute_input":"2021-08-21T22:22:28.381863Z","iopub.status.idle":"2021-08-21T22:22:28.703739Z","shell.execute_reply.started":"2021-08-21T22:22:28.381824Z","shell.execute_reply":"2021-08-21T22:22:28.702704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doc and Word Embeddings","metadata":{}},{"cell_type":"code","source":"tmp = model.layer1.embedding.numpy()\nword_embeddings = tmp[train_size: adj.shape[0] - test_size]\ntrain_doc_embeddings = tmp[:train_size]  # include val docs\ntest_doc_embeddings = tmp[adj.shape[0] - test_size:]\n\nprint_log('Embeddings:')\nprint_log('\\rWord_embeddings:'+str(len(word_embeddings)))\nprint_log('\\rTrain_doc_embeddings:'+str(len(train_doc_embeddings))) \nprint_log('\\rTest_doc_embeddings:'+str(len(test_doc_embeddings))) \nprint_log('\\rWord_embeddings:') \nprint(word_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:16:46.877076Z","iopub.execute_input":"2021-08-21T17:16:46.87744Z","iopub.status.idle":"2021-08-21T17:16:46.8867Z","shell.execute_reply.started":"2021-08-21T17:16:46.877407Z","shell.execute_reply":"2021-08-21T17:16:46.884907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectors = []\nfor i in range(vocab_size):\n    word = vocab[i].strip()\n    word_vector = word_embeddings[i]\n    word_vector_str = ' '.join([str(x) for x in word_vector])\n    word_vectors.append(word + ' ' + word_vector_str)\n\nword_embeddings_str = '\\n'.join(word_vectors)\n#print(word_vectors[0])\n\ndoc_vectors = []\ndoc_id = 0\nfor i in range(train_size):\n    doc_vector = train_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\nfor i in range(test_size):\n    doc_vector = test_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\ndoc_embeddings_str = '\\n'.join(doc_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:16:48.480349Z","iopub.execute_input":"2021-08-21T17:16:48.480741Z","iopub.status.idle":"2021-08-21T17:16:49.566183Z","shell.execute_reply.started":"2021-08-21T17:16:48.480709Z","shell.execute_reply":"2021-08-21T17:16:49.564997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}